{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ext_app_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S_F8IN0SFmd",
        "colab_type": "text"
      },
      "source": [
        "Emailid - rachitrawat18.18je0663@pe.iitism.ac.in\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru-yKhWDSkyb",
        "colab_type": "text"
      },
      "source": [
        
        "Extractive method: There is no parameter learning in this method just the most important sentences in the document are picked up on the basis of scores, which combines to form the summary without losing context of the language.\n",
        " \n",
        "**Function details:**\n",
        " \n",
        "Text_summarizer function has been built to use it as an API. The function contains several secondary functions used for text-preprocessing,\n",
        "calculating TF-IDF scores and calculating sentence score.\n",
        " \n",
        "**parameters:**\n",
        " \n",
        "Text = text string(can be of any length)\n",
        " \n",
        "percentage = int(0-100), default is 50, percentage parameter decides length of the summary returned.\n",
        " \n",
        "**modules required:**required modules can be imported/downloaded by running the cell below.\n",
        " \n",
        "**Corpus used:**Nltk packaged is utilised here for corpus and text-preprocessing as stemming and lemmatization are necessary before creating scores.\n",
        " \n",
        "**Method used:**TF-IDF\n",
        " \n",
        "This particular method uses TF-IDF scores that are alloted to all the words. To give a sentence score, scores of words present in a specific sentence are added. Finally, sentences are arranged in order of scores and top few are picked up to generate summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxPGRNFPo1Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the Required Libraries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import operator\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "wordlemmatizer = WordNetLemmatizer()\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCDYpFl5dIun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5fe91cb9-34e5-439d-b7ce-fe804e42838a"
      },
      "source": [
        "# Reading text file and calling text_summarizer to get summary\n",
        "df = pd.read_excel('TASK1.xlsx')\n",
        "text = df['product descriptons'][20]\n",
        "\n",
        "# Calling text_summarizer to get summary and topic of the product description\n",
        "topic, summary = Text_summarizer(text, percentage = 40)\n",
        "print(topic+':')\n",
        "print(\"\\n\")\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Androanagen:\n",
            "\n",
            "\n",
            "It works by increasing blood flow to the hair follicles which further prevents hair loss and stimulates re-growth resulting in longer, thicker and increased numbers of hair.Androanagen Solution should only be applied directly to the scalp area in the amount, and in the way, specified on the label or by your doctor. This medication must be used continuously to maintain hair growth. This should not happen if you use it correctly and avoid contact with broken skin. If you get it in your eyes, mouth or broken skin, rinse thoroughly with plenty of water. Consult your doctor if you are breastfeeding mother.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqf6gpmxW7F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Text_summarizer(text, percentage = 50):\n",
        "  # Lemmatization\n",
        "  def lemmatize_words(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
        "    return lemmatized_words\n",
        "\n",
        "  # Stemming to get only root words \n",
        "  def stem_words(words):\n",
        "      stemmed_words = []\n",
        "      for word in words:\n",
        "        stemmed_words.append(stemmer.stem(word))\n",
        "      return stemmed_words\n",
        "\n",
        "  # Removing special characters\n",
        "  def remove_special_characters(text):\n",
        "      regex = r'[^a-zA-Z0-9\\s]'\n",
        "      text = re.sub(regex,'',text)\n",
        "      return text\n",
        "\n",
        "  # Calculating Word frequency    \n",
        "  def freq(words):\n",
        "      words = [word.lower() for word in words]\n",
        "      dict_freq = {}\n",
        "      words_unique = []\n",
        "      for word in words:\n",
        "        if word not in words_unique:\n",
        "            words_unique.append(word)\n",
        "      for word in words_unique:\n",
        "        dict_freq[word] = words.count(word)\n",
        "      return dict_freq\n",
        "\n",
        "  # part-of-speech tagging    \n",
        "  def pos_tagging(text):\n",
        "      pos_tag = nltk.pos_tag(text.split())\n",
        "      pos_tagged_noun_verb = []\n",
        "      for word,tag in pos_tag:\n",
        "          if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
        "              pos_tagged_noun_verb.append(word)\n",
        "      return pos_tagged_noun_verb\n",
        "\n",
        "  # Calculating term-frequency    \n",
        "  def tf_score(word,sentence):\n",
        "      freq_sum = 0\n",
        "      word_frequency_in_sentence = 0\n",
        "      len_sentence = len(sentence)\n",
        "      for word_in_sentence in sentence.split():\n",
        "          if word == word_in_sentence:\n",
        "              word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
        "      tf =  word_frequency_in_sentence/ len_sentence\n",
        "      return tf\n",
        "\n",
        "  # Calculating IDF    \n",
        "  def idf_score(no_of_sentences,word,sentences):\n",
        "      no_of_sentence_containing_word = 0\n",
        "      for sentence in sentences:\n",
        "          sentence = remove_special_characters(str(sentence))\n",
        "          sentence = re.sub(r'\\d+', '', sentence)\n",
        "          sentence = sentence.split()\n",
        "          sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
        "          sentence = [word.lower() for word in sentence]\n",
        "          sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
        "          if word in sentence:\n",
        "              no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
        "      idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
        "      return idf\n",
        "\n",
        "  # Calculating tf-idf    \n",
        "  def tf_idf_score(tf,idf):\n",
        "      return tf*idf\n",
        "\n",
        "  # getting tf-idf of a word    \n",
        "  def word_tfidf(dict_freq,word,sentences,sentence):\n",
        "      word_tfidf = []\n",
        "      tf = tf_score(word,sentence)\n",
        "      idf = idf_score(len(sentences),word,sentences)\n",
        "      tf_idf = tf_idf_score(tf,idf)\n",
        "      return tf_idf\n",
        "\n",
        "  # getting sentence score    \n",
        "  def sentence_importance(sentence,dict_freq,sentences):\n",
        "      sentence_score = 0\n",
        "      sentence = remove_special_characters(str(sentence)) \n",
        "      sentence = re.sub(r'\\d+', '', sentence)\n",
        "      pos_tagged_sentence = [] \n",
        "      no_of_sentences = len(sentences)\n",
        "      pos_tagged_sentence = pos_tagging(sentence)\n",
        "      for word in pos_tagged_sentence:\n",
        "            if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
        "                  word = word.lower()\n",
        "                  word = wordlemmatizer.lemmatize(word)\n",
        "                  sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
        "      return sentence_score\n",
        "  \n",
        "  # Getting topic\n",
        "  splitted = text.split()\n",
        "  topic = splitted[0]\n",
        "\n",
        "  # Text preprocessing\n",
        "  tokenized_sentence = tokenize.sent_tokenize(text)\n",
        "  text = remove_special_characters(str(text))\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  tokenized_words_with_stopwords = word_tokenize(text)\n",
        "  tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
        "  tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
        "  tokenized_words = [word.lower() for word in tokenized_words]\n",
        "  tokenized_words = lemmatize_words(tokenized_words)\n",
        "\n",
        "  # word frequency\n",
        "  word_freq = freq(tokenized_words)\n",
        "\n",
        "  input_user = percentage\n",
        "  no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
        "\n",
        "  c = 1\n",
        "  sentence_with_importance = {}\n",
        "  for sent in tokenized_sentence:\n",
        "      sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
        "      sentence_with_importance[c] = sentenceimp\n",
        "      c = c+1\n",
        "\n",
        "  sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
        "\n",
        "  cnt = 0\n",
        "  summary = []\n",
        "  sentence_no = []\n",
        "  for word_prob in sentence_with_importance:\n",
        "      if cnt < no_of_sentences:\n",
        "          sentence_no.append(word_prob[0])\n",
        "          cnt = cnt+1\n",
        "      else:\n",
        "        break\n",
        "  sentence_no.sort()\n",
        "  cnt = 1\n",
        "  for sentence in tokenized_sentence:\n",
        "      if cnt in sentence_no:\n",
        "        summary.append(sentence)\n",
        "      cnt = cnt+1\n",
        "  summary = \" \".join(summary)\n",
        "  \n",
        "  return topic,summary"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
